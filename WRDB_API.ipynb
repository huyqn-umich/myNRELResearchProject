{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFLG+X/T5PeG2WIbKmF/V8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huyqn-umich/myNRELResearchProject/blob/main/WRDB_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFclTXilLKN3",
        "outputId": "d17faa4d-4be6-4d4c-9240-d6bb94952347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing name: 2020\n",
            "Making request for point group 1 of 1...\n",
            "Successfully located the .csv file: /content/extracted_folder/5d2b1a8ffd9cd151c3e6994cc1200976/193879_42.30_-83.69_2020.csv\n",
            "Processed\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import time\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "\n",
        "API_KEY = \"mXHYEn2lt6ZWIMgZzAzQoCW1h8Q6vd8DvxT8AD04\"\n",
        "EMAIL = \"huyqn180295@gmail.com\"\n",
        "BASE_URL = \"https://developer.nrel.gov/api/wind-toolkit/v2/wind/offshore-great-lakes-download.json?\"\n",
        "POINTS = [\n",
        "'193879'\n",
        "]\n",
        "\n",
        "def main():\n",
        "    input_data = {\n",
        "        'attributes': 'friction_velocity_2m,inversemoninobukhovlength_2m,skin_temperature,roughness_length,surface_heat_flux,surface_sea_temperature,pressure_0m,pressure_100m,pressure_200m,pressure_300m,relativehumidity_2m,precipitationrate_0m,temperature_100m,turbulent_kinetic_energy_100m,winddirection_100m,windspeed_100m,temperature_10m,turbulent_kinetic_energy_10m,winddirection_10m,windspeed_10m,temperature_120m,turbulent_kinetic_energy_120m,winddirection_120m,windspeed_120m,temperature_140m,turbulent_kinetic_energy_140m,winddirection_140m,windspeed_140m,temperature_160m,turbulent_kinetic_energy_160m,winddirection_160m,windspeed_160m,temperature_180m,turbulent_kinetic_energy_180m,winddirection_180m,windspeed_180m,temperature_200m,turbulent_kinetic_energy_200m,winddirection_200m,windspeed_200m,temperature_20m,turbulent_kinetic_energy_20m,winddirection_20m,windspeed_20m,temperature_220m,turbulent_kinetic_energy_220m,winddirection_220m,windspeed_220m,temperature_240m,turbulent_kinetic_energy_240m,winddirection_240m,windspeed_240m,temperature_260m,turbulent_kinetic_energy_260m,winddirection_260m,windspeed_260m,temperature_280m,turbulent_kinetic_energy_280m,winddirection_280m,windspeed_280m,temperature_2m,temperature_300m,turbulent_kinetic_energy_300m,winddirection_300m,windspeed_300m,temperature_400m,turbulent_kinetic_energy_400m,winddirection_400m,windspeed_400m,temperature_40m,turbulent_kinetic_energy_40m,winddirection_40m,windspeed_40m,temperature_500m,turbulent_kinetic_energy_500m,winddirection_500m,windspeed_500m,temperature_60m,turbulent_kinetic_energy_60m,winddirection_60m,windspeed_60m,temperature_80m,turbulent_kinetic_energy_80m,winddirection_80m,windspeed_80m',\n",
        "        'interval': '60',\n",
        "        'to_utc': 'false',\n",
        "        'include_leap_day': 'true',\n",
        "\n",
        "        'api_key': API_KEY,\n",
        "        'email': EMAIL,\n",
        "    }\n",
        "    for name in ['2020']:\n",
        "        print(f\"Processing name: {name}\")\n",
        "        for id, location_ids in enumerate(POINTS):\n",
        "            input_data['names'] = [name]\n",
        "            input_data['location_ids'] = location_ids\n",
        "            print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
        "\n",
        "            if '.csv' in BASE_URL:\n",
        "                url = BASE_URL + urllib.parse.urlencode(data, True)\n",
        "                # Note: CSV format is only supported for single point requests\n",
        "                # Suggest that you might append to a larger data frame\n",
        "                data = pd.read_csv(url)\n",
        "                print(f'Response data (you should replace this print statement with your processing): {data}')\n",
        "                # You can use the following code to write it to a file\n",
        "                # data.to_csv('SingleBigDataPoint.csv')\n",
        "            else:\n",
        "                headers = {\n",
        "                  'x-api-key': API_KEY\n",
        "                }\n",
        "                data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
        "                download_url = data['outputs']['downloadUrl']\n",
        "                # Display path to the .csv file\n",
        "                csv_path = locate_csv_file(download_url)\n",
        "                print(f'Successfully located the .csv file: {csv_path}')\n",
        "                # Delay for 1 second to prevent rate limiting\n",
        "                time.sleep(1)\n",
        "            print(f'Processed')\n",
        "\n",
        "\n",
        "def locate_csv_file(url):\n",
        "    # Download the zip file\n",
        "    zip_path = '/content/zip_folder.zip'\n",
        "    response = requests.get(url)\n",
        "    with open(zip_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Extract the zip file\n",
        "    extract_path = '/content/extracted_folder'\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Identify the CSV file path in the extracted folder\n",
        "    csv_path = glob.glob(os.path.join(extract_path, '**/*.csv'))[0]\n",
        "    return csv_path\n",
        "\n",
        "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
        "    \"\"\"Takes the given response and handles any errors, along with providing\n",
        "    the resulting json\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    response : requests.Response\n",
        "        The response object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        The resulting json\n",
        "    \"\"\"\n",
        "    if response.status_code != 200:\n",
        "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
        "        print(f\"The response body: {response.text}\")\n",
        "        exit(1)\n",
        "\n",
        "    try:\n",
        "        response_json = response.json()\n",
        "    except:\n",
        "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
        "        exit(1)\n",
        "\n",
        "    if len(response_json['errors']) > 0:\n",
        "        errors = '\\n'.join(response_json['errors'])\n",
        "        print(f\"The request errored out, here are the errors: {errors}\")\n",
        "        exit(1)\n",
        "    return response_json\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fB4AQTcchsnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}